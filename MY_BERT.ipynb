{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Using cached sentencepiece-0.1.91-cp37-cp37m-win_amd64.whl (1.2 MB)\n",
      "Processing c:\\users\\acer\\appdata\\local\\pip\\cache\\wheels\\69\\09\\d1\\bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\\sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf in c:\\users\\acer\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (1.19.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Using cached tokenizers-0.9.3-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: six in c:\\users\\acer\\appdata\\roaming\\python\\python37\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\acer\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Installing collected packages: sentencepiece, regex, sacremoses, tokenizers, transformers\n",
      "Successfully installed regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\acer\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.2\n",
      "    - pytorch\n",
      "    - torchaudio\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.9.2                |   py37haa95532_0         2.9 MB\n",
      "    cudatoolkit-10.2.89        |       h74a9793_1       317.2 MB\n",
      "    libuv-1.40.0               |       he774522_0         255 KB\n",
      "    ninja-1.10.1               |   py37h7ef1ec2_0         249 KB\n",
      "    pytorch-1.7.0              |py3.7_cuda102_cudnn7_0       767.6 MB  pytorch\n",
      "    torchaudio-0.7.0           |             py37         2.7 MB  pytorch\n",
      "    torchvision-0.8.1          |       py37_cu102         7.2 MB  pytorch\n",
      "    typing_extensions-3.7.4.3  |             py_0          28 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.07 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cudatoolkit        pkgs/main/win-64::cudatoolkit-10.2.89-h74a9793_1\n",
      "  libuv              pkgs/main/win-64::libuv-1.40.0-he774522_0\n",
      "  ninja              pkgs/main/win-64::ninja-1.10.1-py37h7ef1ec2_0\n",
      "  pytorch            pytorch/win-64::pytorch-1.7.0-py3.7_cuda102_cudnn7_0\n",
      "  torchaudio         pytorch/win-64::torchaudio-0.7.0-py37\n",
      "  torchvision        pytorch/win-64::torchvision-0.8.1-py37_cu102\n",
      "  typing_extensions  pkgs/main/noarch::typing_extensions-3.7.4.3-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                                        4.8.2-py37_0 --> 4.9.2-py37haa95532_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "torchaudio-0.7.0     | 2.7 MB    |            |   0% \n",
      "torchaudio-0.7.0     | 2.7 MB    |            |   1% \n",
      "torchaudio-0.7.0     | 2.7 MB    | 1          |   2% \n",
      "torchaudio-0.7.0     | 2.7 MB    | 3          |   3% \n",
      "torchaudio-0.7.0     | 2.7 MB    | 7          |   7% \n",
      "torchaudio-0.7.0     | 2.7 MB    | #2         |  13% \n",
      "torchaudio-0.7.0     | 2.7 MB    | ##1        |  22% \n",
      "torchaudio-0.7.0     | 2.7 MB    | ###8       |  39% \n",
      "torchaudio-0.7.0     | 2.7 MB    | ######8    |  69% \n",
      "torchaudio-0.7.0     | 2.7 MB    | #######4   |  74% \n",
      "torchaudio-0.7.0     | 2.7 MB    | #######9   |  80% \n",
      "torchaudio-0.7.0     | 2.7 MB    | #########1 |  91% \n",
      "torchaudio-0.7.0     | 2.7 MB    | ########## | 100% \n",
      "\n",
      "libuv-1.40.0         | 255 KB    |            |   0% \n",
      "libuv-1.40.0         | 255 KB    | ########## | 100% \n",
      "\n",
      "ninja-1.10.1         | 249 KB    |            |   0% \n",
      "ninja-1.10.1         | 249 KB    | ########## | 100% \n",
      "\n",
      "conda-4.9.2          | 2.9 MB    |            |   0% \n",
      "conda-4.9.2          | 2.9 MB    | ########7  |  88% \n",
      "conda-4.9.2          | 2.9 MB    | ########## | 100% \n",
      "\n",
      "torchvision-0.8.1    | 7.2 MB    |            |   0% \n",
      "torchvision-0.8.1    | 7.2 MB    |            |   0% \n",
      "torchvision-0.8.1    | 7.2 MB    | 6          |   6% \n",
      "torchvision-0.8.1    | 7.2 MB    | #2         |  12% \n",
      "torchvision-0.8.1    | 7.2 MB    | #8         |  18% \n",
      "torchvision-0.8.1    | 7.2 MB    | ##4        |  24% \n",
      "torchvision-0.8.1    | 7.2 MB    | ###        |  30% \n",
      "torchvision-0.8.1    | 7.2 MB    | ###6       |  36% \n",
      "torchvision-0.8.1    | 7.2 MB    | ####2      |  42% \n",
      "torchvision-0.8.1    | 7.2 MB    | ####8      |  49% \n",
      "torchvision-0.8.1    | 7.2 MB    | #####4     |  54% \n",
      "torchvision-0.8.1    | 7.2 MB    | ######     |  61% \n",
      "torchvision-0.8.1    | 7.2 MB    | ######6    |  67% \n",
      "torchvision-0.8.1    | 7.2 MB    | #######2   |  73% \n",
      "torchvision-0.8.1    | 7.2 MB    | #######8   |  79% \n",
      "torchvision-0.8.1    | 7.2 MB    | ########4  |  85% \n",
      "torchvision-0.8.1    | 7.2 MB    | #########  |  91% \n",
      "torchvision-0.8.1    | 7.2 MB    | #########7 |  97% \n",
      "torchvision-0.8.1    | 7.2 MB    | ########## | 100% \n",
      "\n",
      "cudatoolkit-10.2.89  | 317.2 MB  |            |   0% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  |            |   1% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 1          |   1% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 2          |   2% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 2          |   3% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 3          |   3% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 4          |   4% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 4          |   5% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 5          |   6% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 6          |   6% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 6          |   7% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 7          |   8% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 8          |   8% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 9          |   9% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 9          |  10% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #          |  10% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #1         |  11% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #1         |  12% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #2         |  13% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #3         |  13% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #4         |  14% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #4         |  15% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #5         |  15% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #6         |  16% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #7         |  17% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #7         |  18% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #8         |  19% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #9         |  20% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##         |  21% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##1        |  22% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##2        |  22% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##3        |  23% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##4        |  24% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##5        |  25% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##6        |  26% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##7        |  27% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##8        |  28% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##8        |  29% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##9        |  30% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###        |  31% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###1       |  32% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###2       |  33% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###3       |  33% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###4       |  34% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###5       |  35% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###6       |  36% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###7       |  37% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###7       |  38% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###8       |  39% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###9       |  40% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####       |  41% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####1      |  42% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####2      |  43% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####3      |  43% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####4      |  44% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####5      |  45% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####6      |  46% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####7      |  47% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####7      |  48% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####8      |  49% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####9      |  50% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####      |  51% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####1     |  52% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####2     |  52% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####3     |  53% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####4     |  54% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####5     |  55% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####6     |  56% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####6     |  57% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####7     |  58% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####8     |  59% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####9     |  60% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######     |  61% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######1    |  62% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######2    |  63% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######3    |  64% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######4    |  64% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######5    |  65% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######6    |  66% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######7    |  67% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######8    |  68% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######9    |  69% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######    |  70% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######1   |  71% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######1   |  72% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######2   |  73% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######3   |  74% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######4   |  75% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######5   |  76% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######6   |  77% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######7   |  78% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######8   |  79% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######9   |  79% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########   |  80% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########1  |  81% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########2  |  82% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########3  |  83% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########3  |  84% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########4  |  85% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########5  |  86% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########6  |  87% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########7  |  88% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########8  |  88% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########9  |  89% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########  |  90% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########  |  91% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########1 |  92% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########2 |  93% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########3 |  94% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########4 |  95% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########5 |  95% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########6 |  96% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########7 |  97% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########8 |  98% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########8 |  99% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########9 | 100% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########## | 100% \n",
      "\n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   0% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   1% \n",
      "pytorch-1.7.0        | 767.6 MB  |            |   1% \n",
      "pytorch-1.7.0        | 767.6 MB  | 1          |   1% \n",
      "pytorch-1.7.0        | 767.6 MB  | 1          |   1% \n",
      "pytorch-1.7.0        | 767.6 MB  | 1          |   2% \n",
      "pytorch-1.7.0        | 767.6 MB  | 1          |   2% \n",
      "pytorch-1.7.0        | 767.6 MB  | 2          |   2% \n",
      "pytorch-1.7.0        | 767.6 MB  | 2          |   2% \n",
      "pytorch-1.7.0        | 767.6 MB  | 2          |   3% \n",
      "pytorch-1.7.0        | 767.6 MB  | 3          |   3% \n",
      "pytorch-1.7.0        | 767.6 MB  | 3          |   3% \n",
      "pytorch-1.7.0        | 767.6 MB  | 3          |   4% \n",
      "pytorch-1.7.0        | 767.6 MB  | 3          |   4% \n",
      "pytorch-1.7.0        | 767.6 MB  | 4          |   4% \n",
      "pytorch-1.7.0        | 767.6 MB  | 4          |   5% \n",
      "pytorch-1.7.0        | 767.6 MB  | 4          |   5% \n",
      "pytorch-1.7.0        | 767.6 MB  | 5          |   5% \n",
      "pytorch-1.7.0        | 767.6 MB  | 5          |   6% \n",
      "pytorch-1.7.0        | 767.6 MB  | 5          |   6% \n",
      "pytorch-1.7.0        | 767.6 MB  | 6          |   6% \n",
      "pytorch-1.7.0        | 767.6 MB  | 6          |   6% \n",
      "pytorch-1.7.0        | 767.6 MB  | 6          |   6% \n",
      "pytorch-1.7.0        | 767.6 MB  | 6          |   7% \n",
      "pytorch-1.7.0        | 767.6 MB  | 7          |   7% \n",
      "pytorch-1.7.0        | 767.6 MB  | 7          |   8% \n",
      "pytorch-1.7.0        | 767.6 MB  | 7          |   8% \n",
      "pytorch-1.7.0        | 767.6 MB  | 8          |   8% \n",
      "pytorch-1.7.0        | 767.6 MB  | 8          |   8% \n",
      "pytorch-1.7.0        | 767.6 MB  | 8          |   9% \n",
      "pytorch-1.7.0        | 767.6 MB  | 9          |   9% \n",
      "pytorch-1.7.0        | 767.6 MB  | 9          |   9% \n",
      "pytorch-1.7.0        | 767.6 MB  | 9          |  10% \n",
      "pytorch-1.7.0        | 767.6 MB  | #          |  10% \n",
      "pytorch-1.7.0        | 767.6 MB  | #          |  10% \n",
      "pytorch-1.7.0        | 767.6 MB  | #          |  11% \n",
      "pytorch-1.7.0        | 767.6 MB  | #1         |  11% \n",
      "pytorch-1.7.0        | 767.6 MB  | #1         |  12% \n",
      "pytorch-1.7.0        | 767.6 MB  | #1         |  12% \n",
      "pytorch-1.7.0        | 767.6 MB  | #2         |  12% \n",
      "pytorch-1.7.0        | 767.6 MB  | #2         |  13% \n",
      "pytorch-1.7.0        | 767.6 MB  | #2         |  13% \n",
      "pytorch-1.7.0        | 767.6 MB  | #3         |  13% \n",
      "pytorch-1.7.0        | 767.6 MB  | #3         |  14% \n",
      "pytorch-1.7.0        | 767.6 MB  | #3         |  14% \n",
      "pytorch-1.7.0        | 767.6 MB  | #4         |  14% \n",
      "pytorch-1.7.0        | 767.6 MB  | #4         |  14% \n",
      "pytorch-1.7.0        | 767.6 MB  | #4         |  15% \n",
      "pytorch-1.7.0        | 767.6 MB  | #4         |  15% \n",
      "pytorch-1.7.0        | 767.6 MB  | #5         |  15% \n",
      "pytorch-1.7.0        | 767.6 MB  | #5         |  15% \n",
      "pytorch-1.7.0        | 767.6 MB  | #5         |  16% \n",
      "pytorch-1.7.0        | 767.6 MB  | #5         |  16% \n",
      "pytorch-1.7.0        | 767.6 MB  | #6         |  16% \n",
      "pytorch-1.7.0        | 767.6 MB  | #6         |  17% \n",
      "pytorch-1.7.0        | 767.6 MB  | #6         |  17% \n",
      "pytorch-1.7.0        | 767.6 MB  | #7         |  17% \n",
      "pytorch-1.7.0        | 767.6 MB  | #7         |  17% \n",
      "pytorch-1.7.0        | 767.6 MB  | #7         |  17% \n",
      "pytorch-1.7.0        | 767.6 MB  | #7         |  18% \n",
      "pytorch-1.7.0        | 767.6 MB  | #7         |  18% \n",
      "pytorch-1.7.0        | 767.6 MB  | #8         |  18% \n",
      "pytorch-1.7.0        | 767.6 MB  | #8         |  18% \n",
      "pytorch-1.7.0        | 767.6 MB  | #8         |  19% \n",
      "pytorch-1.7.0        | 767.6 MB  | #8         |  19% \n",
      "pytorch-1.7.0        | 767.6 MB  | #9         |  19% \n",
      "pytorch-1.7.0        | 767.6 MB  | #9         |  19% \n",
      "pytorch-1.7.0        | 767.6 MB  | #9         |  20% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##         |  20% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##         |  20% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##         |  21% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##         |  21% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##1        |  21% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##1        |  21% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##1        |  22% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##1        |  22% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##2        |  22% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##2        |  22% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##2        |  23% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##2        |  23% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##3        |  23% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##3        |  23% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##3        |  24% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##3        |  24% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##4        |  24% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##4        |  24% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##4        |  25% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##4        |  25% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##5        |  25% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##5        |  26% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##5        |  26% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##6        |  26% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##6        |  26% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##6        |  27% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##6        |  27% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##7        |  27% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##7        |  27% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##7        |  28% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##7        |  28% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##8        |  28% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##8        |  28% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##8        |  29% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##8        |  29% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##9        |  29% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##9        |  29% \n",
      "pytorch-1.7.0        | 767.6 MB  | ##9        |  30% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###        |  30% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###        |  30% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###        |  31% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###        |  31% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###1       |  31% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###1       |  31% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###1       |  32% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###2       |  32% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###2       |  32% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###2       |  33% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###2       |  33% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###3       |  33% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###3       |  33% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###3       |  34% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###3       |  34% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###4       |  34% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###4       |  34% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###4       |  35% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###5       |  35% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###5       |  35% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###5       |  36% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###5       |  36% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###6       |  36% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###6       |  36% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###6       |  37% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###6       |  37% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###7       |  37% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###7       |  37% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###7       |  38% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###7       |  38% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###8       |  38% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###8       |  38% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###8       |  39% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###8       |  39% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###9       |  39% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###9       |  40% \n",
      "pytorch-1.7.0        | 767.6 MB  | ###9       |  40% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####       |  40% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####       |  40% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####       |  41% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####       |  41% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####1      |  41% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####1      |  41% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####1      |  42% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####1      |  42% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####2      |  42% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####2      |  42% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####2      |  43% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####3      |  43% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####3      |  43% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####3      |  44% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####3      |  44% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####4      |  44% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####4      |  45% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####4      |  45% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####5      |  45% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####5      |  45% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####5      |  46% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####6      |  46% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####6      |  46% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####6      |  47% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####6      |  47% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####7      |  47% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####7      |  48% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####7      |  48% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####8      |  48% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####8      |  48% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####8      |  49% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####9      |  49% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####9      |  49% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####9      |  50% \n",
      "pytorch-1.7.0        | 767.6 MB  | ####9      |  50% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####      |  50% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####      |  50% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####      |  51% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####1     |  51% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####1     |  51% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####1     |  52% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####1     |  52% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####2     |  52% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####2     |  52% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####2     |  53% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####3     |  53% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####3     |  53% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####3     |  54% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####3     |  54% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####4     |  54% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####4     |  54% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####4     |  55% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####4     |  55% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####5     |  55% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####5     |  56% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####5     |  56% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####6     |  56% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####6     |  56% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####6     |  57% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####6     |  57% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####7     |  57% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####7     |  57% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####7     |  58% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####8     |  58% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####8     |  58% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####8     |  59% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####8     |  59% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####9     |  59% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####9     |  60% \n",
      "pytorch-1.7.0        | 767.6 MB  | #####9     |  60% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######     |  60% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######     |  60% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######     |  61% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######     |  61% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######1    |  61% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######1    |  62% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######1    |  62% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######2    |  62% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######2    |  63% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######2    |  63% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######3    |  63% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######3    |  63% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######3    |  64% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######4    |  64% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######4    |  64% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######4    |  65% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######5    |  65% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######5    |  65% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######5    |  66% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######5    |  66% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######6    |  66% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######6    |  67% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######6    |  67% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######7    |  67% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######7    |  67% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######7    |  68% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######7    |  68% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######8    |  68% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######8    |  69% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######8    |  69% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######9    |  69% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######9    |  69% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######9    |  70% \n",
      "pytorch-1.7.0        | 767.6 MB  | ######9    |  70% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######    |  70% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######    |  70% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######    |  71% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######    |  71% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######1   |  71% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######1   |  71% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######1   |  72% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######1   |  72% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######2   |  72% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######2   |  73% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######2   |  73% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######3   |  73% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######3   |  73% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######3   |  74% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######3   |  74% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######4   |  74% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######4   |  74% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######4   |  75% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######4   |  75% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######5   |  75% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######5   |  75% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######5   |  76% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######6   |  76% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######6   |  76% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######6   |  77% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######6   |  77% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######7   |  77% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######7   |  77% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######7   |  78% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######7   |  78% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######8   |  78% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######8   |  78% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######8   |  79% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######8   |  79% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######9   |  79% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######9   |  79% \n",
      "pytorch-1.7.0        | 767.6 MB  | #######9   |  80% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########   |  80% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########   |  80% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########   |  81% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########   |  81% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########1  |  81% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########1  |  81% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########1  |  82% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########1  |  82% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########2  |  82% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########2  |  82% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########2  |  83% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########2  |  83% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########3  |  83% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########3  |  83% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########3  |  84% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########3  |  84% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########4  |  84% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########4  |  84% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########4  |  85% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########4  |  85% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########5  |  85% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########5  |  85% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########5  |  86% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########5  |  86% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########6  |  86% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########6  |  87% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########6  |  87% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########7  |  87% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########7  |  87% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########7  |  88% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########7  |  88% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########8  |  88% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########8  |  89% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########8  |  89% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########9  |  89% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########9  |  89% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########9  |  90% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########9  |  90% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########  |  90% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########  |  90% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########  |  91% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########  |  91% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########1 |  91% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########1 |  91% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########1 |  92% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########1 |  92% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########2 |  92% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########2 |  92% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########2 |  93% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########3 |  93% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########3 |  93% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########3 |  94% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########3 |  94% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########4 |  94% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########4 |  94% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########4 |  95% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########4 |  95% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########5 |  95% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########5 |  95% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########5 |  96% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########6 |  96% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########6 |  96% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########6 |  97% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########6 |  97% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########7 |  97% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########7 |  97% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########7 |  98% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########7 |  98% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########8 |  98% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########8 |  98% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########8 |  99% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########9 |  99% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########9 |  99% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########9 | 100% \n",
      "pytorch-1.7.0        | 767.6 MB  | #########9 | 100% \n",
      "pytorch-1.7.0        | 767.6 MB  | ########## | 100% \n",
      "\n",
      "typing_extensions-3. | 28 KB     |            |   0% \n",
      "typing_extensions-3. | 28 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(r'data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>labeller</th>\n",
       "      <th>emo_class</th>\n",
       "      <th>seven_emo</th>\n",
       "      <th>intent</th>\n",
       "      <th>topic</th>\n",
       "      <th>keyword_vec</th>\n",
       "      <th>similarity</th>\n",
       "      <th>reviewed</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>‡∏ä‡∏¥‡∏ß</td>\n",
       "      <td>‡∏Å‡πá‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡∏≠‡∏ô‡∏ä‡∏¥‡∏ß ‡∏ô‡∏±‡πà‡∏á‡∏ä‡∏¥‡∏ß ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏¥‡∏ô‡∏¢‡∏±‡∏á‡∏ä‡∏¥‡∏ß :)</td>\n",
       "      <td>None</td>\n",
       "      <td>pos_de</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>general</td>\n",
       "      <td>0.6564798951148987,-1.8736761808395386,0.49881...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03T13:52:59.000+07:00</td>\n",
       "      <td>2019-08-04T08:03:10.000+07:00</td>\n",
       "      <td>http://chitchat.tunejai.com/chats/1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>‡πÑ‡∏°‡πà‡∏ô‡∏≠‡∏ô‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏á‡∏≤‡∏ô‡∏™‡∏¥</td>\n",
       "      <td>‡∏Ç‡∏≠‡∏á‡∏µ‡∏ö‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏≠</td>\n",
       "      <td>None</td>\n",
       "      <td>neu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>general</td>\n",
       "      <td>0.5782301053404808,0.3987545693914096,0.271282...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03T13:52:59.000+07:00</td>\n",
       "      <td>2019-08-04T08:04:00.000+07:00</td>\n",
       "      <td>http://chitchat.tunejai.com/chats/2.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>‡∏≠‡∏∑‡∏°‡∏≤</td>\n",
       "      <td>üôÇ</td>\n",
       "      <td>None</td>\n",
       "      <td>neu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>general</td>\n",
       "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03T13:52:59.000+07:00</td>\n",
       "      <td>2019-08-04T08:04:23.000+07:00</td>\n",
       "      <td>http://chitchat.tunejai.com/chats/3.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>‡∏´‡∏¢‡∏¥‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏≠‡∏∞</td>\n",
       "      <td>‡πÄ‡∏≠‡∏≤‡∏°‡∏∑‡∏≠‡∏´‡∏¢‡∏¥‡∏ö‡∏™‡∏¥</td>\n",
       "      <td>None</td>\n",
       "      <td>neu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>chitchat</td>\n",
       "      <td>general</td>\n",
       "      <td>0.038666918873786926,-0.18005990283563733,-0.1...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03T13:52:59.000+07:00</td>\n",
       "      <td>2019-08-04T08:04:50.000+07:00</td>\n",
       "      <td>http://chitchat.tunejai.com/chats/4.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á sumsung ‡∏Å‡∏±‡∏ö iphone ‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ</td>\n",
       "      <td>‡πÄ‡∏ò‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏á‡∏Å‡πá‡∏ß‡πà‡∏≤‡∏á‡∏±‡πâ‡∏ô‡∏•‡∏∞ ‡∏°‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡∏Ñ‡∏ô‡∏ä‡∏≠‡∏ö</td>\n",
       "      <td>None</td>\n",
       "      <td>neu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>question</td>\n",
       "      <td>general</td>\n",
       "      <td>0.3020523651079698,0.10027491030367938,-0.2324...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03T13:52:59.000+07:00</td>\n",
       "      <td>2019-08-04T08:05:24.000+07:00</td>\n",
       "      <td>http://chitchat.tunejai.com/chats/5.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              question  \\\n",
       "0   1                                   ‡∏ä‡∏¥‡∏ß   \n",
       "1   2                     ‡πÑ‡∏°‡πà‡∏ô‡∏≠‡∏ô‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏á‡∏≤‡∏ô‡∏™‡∏¥   \n",
       "2   3                                  ‡∏≠‡∏∑‡∏°‡∏≤   \n",
       "3   4                           ‡∏´‡∏¢‡∏¥‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏≠‡∏∞   \n",
       "4   5  ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á sumsung ‡∏Å‡∏±‡∏ö iphone ‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏î‡∏µ   \n",
       "\n",
       "                                     answer labeller emo_class seven_emo  \\\n",
       "0  ‡∏Å‡πá‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡∏≠‡∏ô‡∏ä‡∏¥‡∏ß ‡∏ô‡∏±‡πà‡∏á‡∏ä‡∏¥‡∏ß ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏¥‡∏ô‡∏¢‡∏±‡∏á‡∏ä‡∏¥‡∏ß :)     None    pos_de   relaxed   \n",
       "1                      ‡∏Ç‡∏≠‡∏á‡∏µ‡∏ö‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏≠     None       neu   neutral   \n",
       "2                                         üôÇ     None       neu   neutral   \n",
       "3                              ‡πÄ‡∏≠‡∏≤‡∏°‡∏∑‡∏≠‡∏´‡∏¢‡∏¥‡∏ö‡∏™‡∏¥     None       neu   neutral   \n",
       "4      ‡πÄ‡∏ò‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏á‡∏Å‡πá‡∏ß‡πà‡∏≤‡∏á‡∏±‡πâ‡∏ô‡∏•‡∏∞ ‡∏°‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡∏Ñ‡∏ô‡∏ä‡∏≠‡∏ö      None       neu   neutral   \n",
       "\n",
       "     intent    topic                                        keyword_vec  \\\n",
       "0  chitchat  general  0.6564798951148987,-1.8736761808395386,0.49881...   \n",
       "1  chitchat  general  0.5782301053404808,0.3987545693914096,0.271282...   \n",
       "2  chitchat  general  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....   \n",
       "3  chitchat  general  0.038666918873786926,-0.18005990283563733,-0.1...   \n",
       "4  question  general  0.3020523651079698,0.10027491030367938,-0.2324...   \n",
       "\n",
       "   similarity  reviewed                     created_at  \\\n",
       "0         1.0      True  2019-06-03T13:52:59.000+07:00   \n",
       "1         1.0      True  2019-06-03T13:52:59.000+07:00   \n",
       "2         1.0      True  2019-06-03T13:52:59.000+07:00   \n",
       "3         1.0      True  2019-06-03T13:52:59.000+07:00   \n",
       "4         1.0      True  2019-06-03T13:52:59.000+07:00   \n",
       "\n",
       "                      updated_at                                       url  \n",
       "0  2019-08-04T08:03:10.000+07:00  http://chitchat.tunejai.com/chats/1.json  \n",
       "1  2019-08-04T08:04:00.000+07:00  http://chitchat.tunejai.com/chats/2.json  \n",
       "2  2019-08-04T08:04:23.000+07:00  http://chitchat.tunejai.com/chats/3.json  \n",
       "3  2019-08-04T08:04:50.000+07:00  http://chitchat.tunejai.com/chats/4.json  \n",
       "4  2019-08-04T08:05:24.000+07:00  http://chitchat.tunejai.com/chats/5.json  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_label = df[['question','seven_emo']]\n",
    "\n",
    "#‡∏Ñ‡πà‡∏≤ label ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None ‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ question ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡πÄ‡∏â‡∏•‡∏¢‡∏Å‡πá‡πÑ‡∏£‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå\n",
    "features_and_label = features_and_label.dropna()\n",
    "\n",
    "#‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ô‡∏∂‡∏á‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 1080 ‡∏ï‡∏±‡∏ß ‡∏ã‡∏∂‡πà‡∏á Mink ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏≤‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á sampling ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 1080 ‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡πÜ ‡∏Å‡∏±‡∏ô\n",
    "angry = features_and_label.loc[features_and_label['seven_emo']=='angry'].sample(1028)\n",
    "bored = features_and_label.loc[features_and_label['seven_emo']=='bored'].sample(1028)\n",
    "depressed = features_and_label.loc[features_and_label['seven_emo']=='depressed'].sample(1028)\n",
    "happy = features_and_label.loc[features_and_label['seven_emo']=='happy'].sample(1028)\n",
    "neutral = features_and_label.loc[features_and_label['seven_emo']=='neutral'].sample(1028)\n",
    "relaxed = features_and_label.loc[features_and_label['seven_emo']=='relaxed'].sample(1028)\n",
    "stressed = features_and_label.loc[features_and_label['seven_emo']=='stressed'].sample(1028)\n",
    "\n",
    "features_and_label = angry\n",
    "features_and_label = features_and_label.append(bored)\n",
    "features_and_label = features_and_label.append(depressed)\n",
    "features_and_label = features_and_label.append(happy)\n",
    "features_and_label = features_and_label.append(neutral)\n",
    "features_and_label = features_and_label.append(relaxed)\n",
    "features_and_label = features_and_label.append(stressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_and_label['question'] #features\n",
    "y = features_and_label['seven_emo'] #labels\n",
    "\n",
    "#‡πÅ‡∏õ‡∏•‡∏á seven_emo ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-6 ‡∏Å‡πà‡∏≠‡∏ô\n",
    "new_y = []\n",
    "for i in y:\n",
    "    if i == 'angry':\n",
    "        new_y.append(0)\n",
    "    elif i == 'bored':\n",
    "        new_y.append(1)\n",
    "    elif i == 'depressed':\n",
    "        new_y.append(2)\n",
    "    elif i == 'happy':\n",
    "        new_y.append(3)\n",
    "    elif i == 'neutral':\n",
    "        new_y.append(4)\n",
    "    elif i == 'relaxed':\n",
    "        new_y.append(5)\n",
    "    else:\n",
    "        new_y.append(6)\n",
    "y = new_y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#‡πÅ‡∏ö‡πà‡∏á train ‡∏Å‡∏±‡∏ö test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#‡πÅ‡∏ö‡πà‡∏á validation ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å train\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train =  3525 ,y_train =  3525 \n",
      " X_val =  1512 ,y_val =  1512\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train = \",len(X_train),\",y_train = \",len(y_train),\"\\n\",\"X_val = \",len(X_val),\",y_val = \",len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a86cbb8874676b0b5dbf92efc23bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e30b84f8dea4fb0ac1be30f3756a744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e813d6c4ed4d1a83d65e400974640b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553a53ef7471474794424d0501e6a245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961828.0, style=ProgressStyle(descript‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 15493, 75890, 37840, 111421, 16000, 30011, 18427, 111413, 23687, 46856, 22598, 46856, 55593, 30011, 20507, 33178, 22123, 111435, 49292, 119, 119, 119, 64175, 37840, 111431, 31256, 80785, 18260, 102058, 111431, 39901, 22765, 22598, 17344, 30011, 45629, 80420, 111435, 17405, 84307, 18260, 33178, 18260, 18427, 30011, 98455, 111435, 49292, 102], [101, 1413, 98455, 46856, 101983, 80420, 46301, 31256, 55593, 31256, 111429, 16000, 18203, 72245, 43102, 22765, 75784, 31256, 19197, 30011, 75784, 111431, 111424, 19197, 17344, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "#‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡∏π‡∏´‡∏ô‡πà‡∏≠‡∏¢\n",
    "text = [\"‡πÅ‡∏•‡∏∞‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ù‡∏ô‡∏ï‡∏Å‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏à‡∏≥‡∏ï‡∏•‡∏≠‡∏î‡πÑ‡∏õ...‡∏ñ‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡πÄ‡∏ò‡∏≠‡πÄ‡∏û‡∏£‡∏∞‡∏≤‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ\",\"‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏Ñ‡∏∑‡∏ô‡∏ó‡∏µ‡πà‡∏ü‡πâ‡∏≤‡∏£‡πâ‡∏≠‡∏á‡∏Ñ‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡∏á‡∏≤\"]\n",
    "\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 7\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    X_val.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(y_train)\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(y_val)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,7)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.01526498 1.01118761 1.02979842 0.95373377 1.00513259 0.98933483\n",
      " 0.99914966]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0153, 1.0112, 1.0298, 0.9537, 1.0051, 0.9893, 0.9991],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.867\n",
      "Validation Loss: 1.849\n",
      "\n",
      " Epoch 2 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.864\n",
      "Validation Loss: 1.868\n",
      "\n",
      " Epoch 3 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.856\n",
      "Validation Loss: 1.841\n",
      "\n",
      " Epoch 4 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.858\n",
      "Validation Loss: 1.840\n",
      "\n",
      " Epoch 5 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.849\n",
      "Validation Loss: 1.848\n",
      "\n",
      " Epoch 6 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.851\n",
      "Validation Loss: 1.863\n",
      "\n",
      " Epoch 7 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.853\n",
      "Validation Loss: 1.849\n",
      "\n",
      " Epoch 8 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.852\n",
      "Validation Loss: 1.838\n",
      "\n",
      " Epoch 9 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.846\n",
      "Validation Loss: 1.845\n",
      "\n",
      " Epoch 10 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.837\n",
      "Validation Loss: 1.846\n",
      "\n",
      " Epoch 11 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.833\n",
      "Validation Loss: 1.850\n",
      "\n",
      " Epoch 12 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.841\n",
      "Validation Loss: 1.833\n",
      "\n",
      " Epoch 13 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.840\n",
      "Validation Loss: 1.829\n",
      "\n",
      " Epoch 14 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.836\n",
      "Validation Loss: 1.836\n",
      "\n",
      " Epoch 15 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.837\n",
      "Validation Loss: 1.823\n",
      "\n",
      " Epoch 16 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.835\n",
      "Validation Loss: 1.836\n",
      "\n",
      " Epoch 17 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.830\n",
      "Validation Loss: 1.833\n",
      "\n",
      " Epoch 18 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.829\n",
      "Validation Loss: 1.814\n",
      "\n",
      " Epoch 19 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.836\n",
      "Validation Loss: 1.833\n",
      "\n",
      " Epoch 20 / 20\n",
      "  Batch    50  of    111.\n",
      "  Batch   100  of    111.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.834\n",
      "Validation Loss: 1.845\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.28      0.29       319\n",
      "           1       0.31      0.41      0.35       316\n",
      "           2       0.28      0.44      0.35       320\n",
      "           3       0.26      0.25      0.26       278\n",
      "           4       0.24      0.22      0.23       304\n",
      "           5       0.33      0.31      0.32       308\n",
      "           6       0.36      0.14      0.20       314\n",
      "\n",
      "    accuracy                           0.29      2159\n",
      "   macro avg       0.30      0.29      0.28      2159\n",
      "weighted avg       0.30      0.29      0.29      2159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
